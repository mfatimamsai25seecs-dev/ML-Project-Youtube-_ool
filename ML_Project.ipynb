{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAmTmAYsFu-p",
        "outputId": "f55644e5-df7a-4f83-9047-980b5f0f8740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "except ModuleNotFoundError:\n",
        "  print(\"Not running on colab...\")\n",
        "\n",
        "RUN_DASHBOARD = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69INXQJ0DNQZ"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JvGRjHVB-ELN",
        "outputId": "ad1628c8-aee0-42a7-b099-ea7c21867569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (2.187.0)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from youtube-transcript-api) (2.32.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.31.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.43.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.2.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.28.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->youtube-transcript-api) (2025.11.12)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api google-api-python-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sk0iKaiDXGx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload # Import needed for downloading and uploading\n",
        "import re # Keep this import as it's used by clean_comment\n",
        "from typing import List, Optional\n",
        "\n",
        "def drive_to_df(folder_id: str, filename: str, column_names: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "    # Find the file ID of 'comments.csv' in the folder_id\n",
        "    query = f\"name='{filename}' and '{folder_id}' in parents\"\n",
        "    response = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()\n",
        "    files = response.get('files', [])\n",
        "\n",
        "    if files:\n",
        "        file_id = files[0].get('id')\n",
        "        print(f\"Found '{filename}' with ID: {file_id}\")\n",
        "\n",
        "        # Download the file content\n",
        "        request = service.files().get_media(fileId=file_id)\n",
        "        fh = BytesIO()\n",
        "        downloader = MediaIoBaseDownload(fh, request)\n",
        "        done = False\n",
        "        while done is False:\n",
        "            status, done = downloader.next_chunk()\n",
        "            print(f\"Download {int(status.progress() * 100)}%.\")\n",
        "\n",
        "        fh.seek(0) # Reset buffer position to the beginning\n",
        "\n",
        "        # Read the downloaded content into a pandas DataFrame\n",
        "        if column_names:\n",
        "            df = pd.read_csv(fh, header=None, names=column_names)\n",
        "        else:\n",
        "            df = pd.read_csv(fh)\n",
        "        print(\"Comments loaded successfully into df.\")\n",
        "    else:\n",
        "        print(f\"Error: '{filename}' not found in folder {folder_id}. Please ensure the file is uploaded.\")\n",
        "        df = pd.DataFrame() # Initialize empty DataFrame to avoid errors later\n",
        "    return df\n",
        "\n",
        "\n",
        "def df_to_drive(df, folder_id, filename):\n",
        "    \"\"\"\n",
        "    Uploads a pandas DataFrame as a CSV file to Google Drive.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to upload.\n",
        "        folder_id (str): The ID of the Google Drive folder where the file will be saved.\n",
        "        filename (str): The desired filename for the CSV file in Google Drive.\n",
        "    \"\"\"\n",
        "    # Convert DataFrame to CSV in-memory\n",
        "    csv_buffer = BytesIO()\n",
        "    df.to_csv(csv_buffer, index=False, encoding='utf-8')\n",
        "    csv_buffer.seek(0) # Rewind the buffer to the beginning\n",
        "\n",
        "    # Prepare metadata for the new file\n",
        "    file_metadata = {\n",
        "        'name': filename,\n",
        "        'parents': [folder_id]\n",
        "    }\n",
        "\n",
        "    # Create a MediaIoBaseUpload object from the BytesIO buffer\n",
        "    media_body = MediaIoBaseUpload(csv_buffer, mimetype='text/csv', resumable=True)\n",
        "\n",
        "    # Upload the file\n",
        "    try:\n",
        "        uploaded_file = service.files().create(\n",
        "            body=file_metadata,\n",
        "            media_body=media_body,\n",
        "            fields='id'\n",
        "        ).execute()\n",
        "        print(f\"Uploaded '{filename}' to Drive with ID: {uploaded_file.get('id')}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error uploading '{filename}' to Drive: {e}\")\n",
        "\n",
        "\n",
        "def clean_comment(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)  # remove links\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)     # remove mentions\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", \" \", text)  # keep punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # remove extra spaces\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSmUZmRVDbdE"
      },
      "outputs": [],
      "source": [
        "DRIVE_FOLDER_ID = '1NEXXEvuBZLB1MFqRCkeGbJfYn4wbA2SX'\n",
        "YOUTUBE_VIDEO_ID = 'zozEm4f_dlw'\n",
        "YOUTUBE_API_KEY = \"AIzaSyBpSCBmvcy9LAQz2Q3cVExHk_y_dbk6zss\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RCVFFs8DHgj"
      },
      "source": [
        "### Set up Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFdgvzYpuAzu"
      },
      "outputs": [],
      "source": [
        "# Connect to Drive\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import google.auth\n",
        "creds, _ = google.auth.default()\n",
        "service = build('drive', 'v3', credentials=creds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aorLUvxlBha3"
      },
      "outputs": [],
      "source": [
        "# Create directory with name: youtube_video_id if not already exists\n",
        "def setup_drive(youtube_video_id, drive_folder_id):\n",
        "    query = f\"name='{youtube_video_id}' and mimeType='application/vnd.google-apps.folder' and '{drive_folder_id}' in parents\"\n",
        "    response = service.files().list(q=query, spaces='drive', fields='files(id, name)').execute()\n",
        "    files = response.get('files', [])\n",
        "\n",
        "    if files:\n",
        "        # Folder already exists\n",
        "        video_folder_id = files[0].get('id')\n",
        "        print(f\"Using existing folder with ID: {video_folder_id}\")\n",
        "    else:\n",
        "        # Create new folder\n",
        "        file_metadata = {\n",
        "            'name': youtube_video_id,\n",
        "            'mimeType': 'application/vnd.google-apps.folder',\n",
        "            'parents': [drive_folder_id]\n",
        "        }\n",
        "        file = service.files().create(body=file_metadata, fields='id').execute()\n",
        "        video_folder_id = file.get('id')\n",
        "        print(f\"Created new folder with ID: {video_folder_id}\")\n",
        "    return video_folder_id\n",
        "\n",
        "if not RUN_DASHBOARD:\n",
        "  VIDEO_FOLDER_ID = setup_drive(YOUTUBE_VIDEO_ID, DRIVE_FOLDER_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3einMu_-skX"
      },
      "source": [
        "### Extract video transcript and save to Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IU7uBv6-u-a"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Optional\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound, VideoUnavailable\n",
        "import pandas as pd # Import pandas for DataFrame creation\n",
        "\n",
        "\n",
        "def fetch_english_transcript(video_id: str) -> Optional[List[Dict]]:\n",
        "    \"\"\"\n",
        "    Fetches the English transcript for a YouTube video.\n",
        "\n",
        "    Preference order:\n",
        "        1. Manually uploaded English transcript\n",
        "        2. Auto-generated English transcript\n",
        "        3. None (if no transcript found or disabled)\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The YouTube video ID (e.g., 'dQw4w9WgXcQ').\n",
        "\n",
        "    Returns:\n",
        "        list[dict] or None: List of transcript segments as dicts like:\n",
        "            [\n",
        "                {\"text\": \"Hello world\", \"start\": 0.0, \"duration\": 3.5},\n",
        "                ...\n",
        "            ]\n",
        "        Returns None if no transcript is available.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize API\n",
        "        ytt_api = YouTubeTranscriptApi()\n",
        "\n",
        "        # Get all available transcripts for this video\n",
        "        transcript_list = ytt_api.list(video_id)\n",
        "\n",
        "        # Try manually created English transcript first\n",
        "        try:\n",
        "            transcript = transcript_list.find_manually_created_transcript(['en'])\n",
        "        except NoTranscriptFound:\n",
        "            # Fallback to auto-generated English transcript\n",
        "            try:\n",
        "                transcript = transcript_list.find_generated_transcript(['en'])\n",
        "            except NoTranscriptFound:\n",
        "                return None  # Neither manual nor auto-generated transcript found\n",
        "\n",
        "        # Fetch the transcript data and return as list of dicts\n",
        "        return transcript.fetch()\n",
        "\n",
        "    except (TranscriptsDisabled, VideoUnavailable):\n",
        "        # Transcripts disabled or video unavailable\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        # Handle unexpected errors safely\n",
        "        print(f\"Error fetching transcript for video {video_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "if not RUN_DASHBOARD:\n",
        "    transcript_data = fetch_english_transcript(YOUTUBE_VIDEO_ID)\n",
        "\n",
        "    if transcript_data:\n",
        "        df_transcript = pd.DataFrame(transcript_data)\n",
        "        print(\"Transcript loaded successfully into DataFrame.\")\n",
        "\n",
        "        # Upload to drive using the existing df_to_drive function\n",
        "        df_to_drive(df_transcript, VIDEO_FOLDER_ID, 'transcript.csv')\n",
        "    else:\n",
        "        print(f\"No English transcript found for video ID: {YOUTUBE_VIDEO_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c62b74f"
      },
      "source": [
        "### Extract video metadata and save to Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de9a36d3"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "# Build the YouTube API service\n",
        "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
        "\n",
        "def get_video_metadata(video_id: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetches video metadata (title, description, published date, thumbnail URL, category ID)\n",
        "    for a given YouTube video ID.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): The YouTube video ID.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the video metadata.\n",
        "    \"\"\"\n",
        "    request = youtube.videos().list(\n",
        "        part=\"snippet\",\n",
        "        id=video_id\n",
        "    )\n",
        "    response = request.execute()\n",
        "\n",
        "    if response['items']:\n",
        "        snippet = response['items'][0]['snippet']\n",
        "        metadata = {\n",
        "            'video_id': video_id,\n",
        "            'title': snippet.get('title'),\n",
        "            'description': snippet.get('description'),\n",
        "            'publishedAt': snippet.get('publishedAt'),\n",
        "            'thumbnail_url': snippet['thumbnails']['default']['url'] if 'thumbnails' in snippet else None,\n",
        "            'categoryId': snippet.get('categoryId')\n",
        "        }\n",
        "        return pd.DataFrame([metadata])\n",
        "    else:\n",
        "        print(f\"No video found with ID: {video_id}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "if not RUN_DASHBOARD:\n",
        "    # Get video metadata\n",
        "    df_metadata = get_video_metadata(YOUTUBE_VIDEO_ID)\n",
        "\n",
        "    if not df_metadata.empty:\n",
        "        print(\"Video metadata loaded successfully into DataFrame.\")\n",
        "        # Upload to drive using the existing df_to_drive function\n",
        "        df_to_drive(df_metadata, VIDEO_FOLDER_ID, 'video_metadata.csv')\n",
        "    else:\n",
        "        print(f\"Failed to retrieve metadata for video ID: {YOUTUBE_VIDEO_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjwWJC2MqNy1"
      },
      "source": [
        "### Extract comments from a youtube video and save to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YSGsZdv8qK--"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd # Import pandas for DataFrame creation\n",
        "\n",
        "def save_youtube_comments_to_drive(\n",
        "    video_id: str,\n",
        "    api_key: str,\n",
        "    service,\n",
        "    drive_folder_id: str,\n",
        "    max_retries: int = 5,\n",
        "    backoff_base: float = 2.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    Fetches YouTube comments for a given video ID and stores them in a pandas DataFrame,\n",
        "    then uploads the DataFrame as a CSV file to a Google Drive folder using df_to_drive.\n",
        "\n",
        "    Args:\n",
        "        video_id (str): YouTube video ID\n",
        "        api_key (str): YouTube Data API v3 key\n",
        "        service: Authenticated Google Drive API service object\n",
        "        drive_folder_id (str): Destination Google Drive folder ID\n",
        "        max_retries (int): Max retries per failed request\n",
        "        backoff_base (float): Base for exponential backoff (seconds)\n",
        "    \"\"\"\n",
        "    base_url = \"https://www.googleapis.com/youtube/v3/commentThreads\"\n",
        "    params = {\n",
        "        \"part\": \"snippet\",\n",
        "        \"videoId\": video_id,\n",
        "        \"maxResults\": 100,\n",
        "        \"textFormat\": \"plainText\",\n",
        "        \"key\": api_key,\n",
        "    }\n",
        "\n",
        "    session = requests.Session()\n",
        "    next_page_token = None\n",
        "    total_comments = 0\n",
        "    all_comments_data = [] # List to store all comment rows\n",
        "\n",
        "    while True:\n",
        "        if next_page_token:\n",
        "            params[\"pageToken\"] = next_page_token\n",
        "        else:\n",
        "            params.pop(\"pageToken\", None)\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                resp = session.get(base_url, params=params, timeout=10)\n",
        "                if resp.status_code == 200:\n",
        "                    data = resp.json()\n",
        "                    break\n",
        "                else:\n",
        "                    print(f\"Warning: HTTP {resp.status_code}, retrying...\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Network error: {e}, retrying...\")\n",
        "\n",
        "            sleep_time = backoff_base ** attempt\n",
        "            print(f\"Sleeping {sleep_time:.1f}s before retry...\")\n",
        "            time.sleep(sleep_time)\n",
        "        else:\n",
        "            raise RuntimeError(\"Max retries exceeded while fetching comments.\")\n",
        "\n",
        "        # Parse comments\n",
        "        items = data.get(\"items\", [])\n",
        "        for item in items:\n",
        "            snippet = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
        "            row = [\n",
        "                snippet.get(\"authorDisplayName\", \"\"),\n",
        "                snippet.get(\"textDisplay\", \"\").replace(\"\\n\", \" \"),\n",
        "                snippet.get(\"likeCount\", 0),\n",
        "                snippet.get(\"publishedAt\", \"\"),\n",
        "            ]\n",
        "            all_comments_data.append(row) # Append row to list\n",
        "            total_comments += 1\n",
        "\n",
        "        print(f\"Fetched {len(items)} comments (total: {total_comments})\")\n",
        "\n",
        "        next_page_token = data.get(\"nextPageToken\")\n",
        "        if not next_page_token:\n",
        "            break\n",
        "\n",
        "        # Be polite â€” small delay to avoid rate limits\n",
        "        time.sleep(0.3)\n",
        "\n",
        "    # Create DataFrame from collected data with explicit column names\n",
        "    df_comments = pd.DataFrame(all_comments_data, columns=['authorDisplayName', 'commentText', 'likeCount', 'publishedAt'])\n",
        "\n",
        "    # Upload the DataFrame to Google Drive using df_to_drive\n",
        "    print(\"Uploading to Google Drive...\")\n",
        "    df_to_drive(df_comments, drive_folder_id, \"comments.csv\")\n",
        "    print(f\"Total comments saved: {total_comments}\")\n",
        "    return df_comments\n",
        "\n",
        "\n",
        "if not RUN_DASHBOARD:\n",
        "    save_youtube_comments_to_drive(\n",
        "        YOUTUBE_VIDEO_ID,\n",
        "        YOUTUBE_API_KEY,\n",
        "        service,\n",
        "        VIDEO_FOLDER_ID\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odI0VqcbIYZg"
      },
      "source": [
        "\n",
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAuE7r5uIeqM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load comments CSV\n",
        "\n",
        "def preprocess_comments(video_folder_id):\n",
        "    df = drive_to_df(video_folder_id, 'comments.csv', column_names=['authorDisplayName', 'commentText', 'likeCount', 'publishedAt'])\n",
        "\n",
        "    df['cleanedCommentText'] = df['commentText'].apply(clean_comment)\n",
        "\n",
        "    # Remove exact duplicates\n",
        "    df = df.drop_duplicates(subset='cleanedCommentText')\n",
        "\n",
        "    # Remove very short comments (e.g. emojis only or less than 3 words)\n",
        "    df = df[df['cleanedCommentText'].str.split().str.len() > 2]\n",
        "\n",
        "    df_to_drive(df, video_folder_id, 'comments_preprocessed.csv')\n",
        "    print(f\"Cleaned comments saved to: {os.path.join(video_folder_id, 'comments_preprocessed.csv')}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "if not RUN_DASHBOARD:\n",
        "    preprocess_comments(VIDEO_FOLDER_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRke79hfmqXf",
        "outputId": "afddb290-b010-4b5e-9045-2a43aa364651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created new folder with ID: 1Ow6TcABf2iDLiiNPgIAcU4Pey7Ub73RZ\n",
            "Transcript loaded successfully into DataFrame.\n",
            "Uploaded 'transcript.csv' to Drive with ID: 1vTANGtkIvf455XolZ9-ghO_FmOb3m1zO\n",
            "Video metadata loaded successfully into DataFrame.\n",
            "Uploaded 'video_metadata.csv' to Drive with ID: 1vxUc0_DcjKUdXzEfkLUuUgQEsI15-KTW\n",
            "Fetched 100 comments (total: 100)\n",
            "Fetched 100 comments (total: 200)\n",
            "Fetched 100 comments (total: 300)\n",
            "Fetched 100 comments (total: 400)\n",
            "Fetched 100 comments (total: 500)\n",
            "Fetched 100 comments (total: 600)\n",
            "Fetched 100 comments (total: 700)\n",
            "Fetched 100 comments (total: 800)\n",
            "Fetched 100 comments (total: 900)\n",
            "Fetched 100 comments (total: 1000)\n",
            "Fetched 100 comments (total: 1100)\n",
            "Fetched 100 comments (total: 1200)\n",
            "Fetched 100 comments (total: 1300)\n",
            "Fetched 100 comments (total: 1400)\n",
            "Fetched 100 comments (total: 1500)\n",
            "Fetched 100 comments (total: 1600)\n",
            "Fetched 100 comments (total: 1700)\n",
            "Fetched 100 comments (total: 1800)\n",
            "Fetched 100 comments (total: 1900)\n",
            "Fetched 100 comments (total: 2000)\n",
            "Fetched 100 comments (total: 2100)\n",
            "Fetched 100 comments (total: 2200)\n",
            "Fetched 2 comments (total: 2202)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 1VxtxzlcIKwBTOp1HZd3d0whM_XFaUoUI\n",
            "Total comments saved: 2202\n",
            "Found 'comments.csv' with ID: 1VxtxzlcIKwBTOp1HZd3d0whM_XFaUoUI\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 1sP1s1J1MuSwhVH0C1aPDAiz1Jaa4qzWf\n",
            "Cleaned comments saved to: 1Ow6TcABf2iDLiiNPgIAcU4Pey7Ub73RZ/comments_preprocessed.csv\n",
            "Created new folder with ID: 1QR9Z19qT6Tj-B1N4bjzc1D-4dteE47k-\n",
            "No English transcript found for video ID: PssKpzB0Ah0\n",
            "Video metadata loaded successfully into DataFrame.\n",
            "Uploaded 'video_metadata.csv' to Drive with ID: 1WXRyfhNnijKPSDhVrQk2am84Yvd5yZaG\n",
            "Fetched 99 comments (total: 99)\n",
            "Fetched 100 comments (total: 199)\n",
            "Fetched 100 comments (total: 299)\n",
            "Fetched 100 comments (total: 399)\n",
            "Fetched 100 comments (total: 499)\n",
            "Fetched 100 comments (total: 599)\n",
            "Fetched 100 comments (total: 699)\n",
            "Fetched 100 comments (total: 799)\n",
            "Fetched 100 comments (total: 899)\n",
            "Fetched 100 comments (total: 999)\n",
            "Fetched 100 comments (total: 1099)\n",
            "Fetched 100 comments (total: 1199)\n",
            "Fetched 100 comments (total: 1299)\n",
            "Fetched 100 comments (total: 1399)\n",
            "Fetched 100 comments (total: 1499)\n",
            "Fetched 100 comments (total: 1599)\n",
            "Fetched 100 comments (total: 1699)\n",
            "Fetched 100 comments (total: 1799)\n",
            "Fetched 100 comments (total: 1899)\n",
            "Fetched 100 comments (total: 1999)\n",
            "Fetched 100 comments (total: 2099)\n",
            "Fetched 100 comments (total: 2199)\n",
            "Fetched 100 comments (total: 2299)\n",
            "Fetched 100 comments (total: 2399)\n",
            "Fetched 100 comments (total: 2499)\n",
            "Fetched 100 comments (total: 2599)\n",
            "Fetched 100 comments (total: 2699)\n",
            "Fetched 100 comments (total: 2799)\n",
            "Fetched 100 comments (total: 2899)\n",
            "Fetched 100 comments (total: 2999)\n",
            "Fetched 100 comments (total: 3099)\n",
            "Fetched 100 comments (total: 3199)\n",
            "Fetched 100 comments (total: 3299)\n",
            "Fetched 100 comments (total: 3399)\n",
            "Fetched 100 comments (total: 3499)\n",
            "Fetched 100 comments (total: 3599)\n",
            "Fetched 100 comments (total: 3699)\n",
            "Fetched 100 comments (total: 3799)\n",
            "Fetched 100 comments (total: 3899)\n",
            "Fetched 100 comments (total: 3999)\n",
            "Fetched 100 comments (total: 4099)\n",
            "Fetched 100 comments (total: 4199)\n",
            "Fetched 100 comments (total: 4299)\n",
            "Fetched 100 comments (total: 4399)\n",
            "Fetched 100 comments (total: 4499)\n",
            "Fetched 100 comments (total: 4599)\n",
            "Fetched 100 comments (total: 4699)\n",
            "Fetched 100 comments (total: 4799)\n",
            "Fetched 100 comments (total: 4899)\n",
            "Fetched 100 comments (total: 4999)\n",
            "Fetched 100 comments (total: 5099)\n",
            "Fetched 100 comments (total: 5199)\n",
            "Fetched 100 comments (total: 5299)\n",
            "Fetched 100 comments (total: 5399)\n",
            "Fetched 100 comments (total: 5499)\n",
            "Fetched 100 comments (total: 5599)\n",
            "Fetched 100 comments (total: 5699)\n",
            "Fetched 100 comments (total: 5799)\n",
            "Fetched 100 comments (total: 5899)\n",
            "Fetched 100 comments (total: 5999)\n",
            "Fetched 100 comments (total: 6099)\n",
            "Fetched 100 comments (total: 6199)\n",
            "Fetched 100 comments (total: 6299)\n",
            "Fetched 100 comments (total: 6399)\n",
            "Fetched 100 comments (total: 6499)\n",
            "Fetched 100 comments (total: 6599)\n",
            "Fetched 100 comments (total: 6699)\n",
            "Fetched 100 comments (total: 6799)\n",
            "Fetched 100 comments (total: 6899)\n",
            "Fetched 100 comments (total: 6999)\n",
            "Fetched 100 comments (total: 7099)\n",
            "Fetched 100 comments (total: 7199)\n",
            "Fetched 100 comments (total: 7299)\n",
            "Fetched 100 comments (total: 7399)\n",
            "Fetched 100 comments (total: 7499)\n",
            "Fetched 100 comments (total: 7599)\n",
            "Fetched 100 comments (total: 7699)\n",
            "Fetched 100 comments (total: 7799)\n",
            "Fetched 100 comments (total: 7899)\n",
            "Fetched 100 comments (total: 7999)\n",
            "Fetched 100 comments (total: 8099)\n",
            "Fetched 100 comments (total: 8199)\n",
            "Fetched 100 comments (total: 8299)\n",
            "Fetched 100 comments (total: 8399)\n",
            "Fetched 100 comments (total: 8499)\n",
            "Fetched 100 comments (total: 8599)\n",
            "Fetched 100 comments (total: 8699)\n",
            "Fetched 100 comments (total: 8799)\n",
            "Fetched 100 comments (total: 8899)\n",
            "Fetched 100 comments (total: 8999)\n",
            "Fetched 100 comments (total: 9099)\n",
            "Fetched 100 comments (total: 9199)\n",
            "Fetched 100 comments (total: 9299)\n",
            "Fetched 100 comments (total: 9399)\n",
            "Fetched 100 comments (total: 9499)\n",
            "Fetched 100 comments (total: 9599)\n",
            "Fetched 100 comments (total: 9699)\n",
            "Fetched 100 comments (total: 9799)\n",
            "Fetched 100 comments (total: 9899)\n",
            "Fetched 100 comments (total: 9999)\n",
            "Fetched 100 comments (total: 10099)\n",
            "Fetched 100 comments (total: 10199)\n",
            "Fetched 100 comments (total: 10299)\n",
            "Fetched 100 comments (total: 10399)\n",
            "Fetched 100 comments (total: 10499)\n",
            "Fetched 100 comments (total: 10599)\n",
            "Fetched 100 comments (total: 10699)\n",
            "Fetched 100 comments (total: 10799)\n",
            "Fetched 100 comments (total: 10899)\n",
            "Fetched 100 comments (total: 10999)\n",
            "Fetched 100 comments (total: 11099)\n",
            "Fetched 100 comments (total: 11199)\n",
            "Fetched 100 comments (total: 11299)\n",
            "Fetched 100 comments (total: 11399)\n",
            "Fetched 100 comments (total: 11499)\n",
            "Fetched 100 comments (total: 11599)\n",
            "Fetched 100 comments (total: 11699)\n",
            "Fetched 100 comments (total: 11799)\n",
            "Fetched 100 comments (total: 11899)\n",
            "Fetched 100 comments (total: 11999)\n",
            "Fetched 100 comments (total: 12099)\n",
            "Fetched 100 comments (total: 12199)\n",
            "Fetched 100 comments (total: 12299)\n",
            "Fetched 100 comments (total: 12399)\n",
            "Fetched 100 comments (total: 12499)\n",
            "Fetched 100 comments (total: 12599)\n",
            "Fetched 100 comments (total: 12699)\n",
            "Fetched 100 comments (total: 12799)\n",
            "Fetched 100 comments (total: 12899)\n",
            "Fetched 100 comments (total: 12999)\n",
            "Fetched 100 comments (total: 13099)\n",
            "Fetched 100 comments (total: 13199)\n",
            "Fetched 100 comments (total: 13299)\n",
            "Fetched 100 comments (total: 13399)\n",
            "Fetched 100 comments (total: 13499)\n",
            "Fetched 100 comments (total: 13599)\n",
            "Fetched 100 comments (total: 13699)\n",
            "Fetched 100 comments (total: 13799)\n",
            "Fetched 100 comments (total: 13899)\n",
            "Fetched 100 comments (total: 13999)\n",
            "Fetched 100 comments (total: 14099)\n",
            "Fetched 100 comments (total: 14199)\n",
            "Fetched 100 comments (total: 14299)\n",
            "Fetched 100 comments (total: 14399)\n",
            "Fetched 100 comments (total: 14499)\n",
            "Fetched 100 comments (total: 14599)\n",
            "Fetched 100 comments (total: 14699)\n",
            "Fetched 100 comments (total: 14799)\n",
            "Fetched 100 comments (total: 14899)\n",
            "Fetched 100 comments (total: 14999)\n",
            "Fetched 100 comments (total: 15099)\n",
            "Fetched 100 comments (total: 15199)\n",
            "Fetched 100 comments (total: 15299)\n",
            "Fetched 100 comments (total: 15399)\n",
            "Fetched 100 comments (total: 15499)\n",
            "Fetched 100 comments (total: 15599)\n",
            "Fetched 100 comments (total: 15699)\n",
            "Fetched 100 comments (total: 15799)\n",
            "Fetched 100 comments (total: 15899)\n",
            "Fetched 100 comments (total: 15999)\n",
            "Fetched 100 comments (total: 16099)\n",
            "Fetched 100 comments (total: 16199)\n",
            "Fetched 100 comments (total: 16299)\n",
            "Fetched 100 comments (total: 16399)\n",
            "Fetched 100 comments (total: 16499)\n",
            "Fetched 100 comments (total: 16599)\n",
            "Fetched 100 comments (total: 16699)\n",
            "Fetched 100 comments (total: 16799)\n",
            "Fetched 100 comments (total: 16899)\n",
            "Fetched 100 comments (total: 16999)\n",
            "Fetched 100 comments (total: 17099)\n",
            "Fetched 100 comments (total: 17199)\n",
            "Fetched 100 comments (total: 17299)\n",
            "Fetched 100 comments (total: 17399)\n",
            "Fetched 100 comments (total: 17499)\n",
            "Fetched 100 comments (total: 17599)\n",
            "Fetched 100 comments (total: 17699)\n",
            "Fetched 100 comments (total: 17799)\n",
            "Fetched 100 comments (total: 17899)\n",
            "Fetched 100 comments (total: 17999)\n",
            "Fetched 100 comments (total: 18099)\n",
            "Fetched 100 comments (total: 18199)\n",
            "Fetched 100 comments (total: 18299)\n",
            "Fetched 100 comments (total: 18399)\n",
            "Fetched 100 comments (total: 18499)\n",
            "Fetched 100 comments (total: 18599)\n",
            "Fetched 100 comments (total: 18699)\n",
            "Fetched 100 comments (total: 18799)\n",
            "Fetched 100 comments (total: 18899)\n",
            "Fetched 100 comments (total: 18999)\n",
            "Fetched 100 comments (total: 19099)\n",
            "Fetched 100 comments (total: 19199)\n",
            "Fetched 100 comments (total: 19299)\n",
            "Fetched 100 comments (total: 19399)\n",
            "Fetched 100 comments (total: 19499)\n",
            "Fetched 100 comments (total: 19599)\n",
            "Fetched 100 comments (total: 19699)\n",
            "Fetched 100 comments (total: 19799)\n",
            "Fetched 100 comments (total: 19899)\n",
            "Fetched 100 comments (total: 19999)\n",
            "Fetched 100 comments (total: 20099)\n",
            "Fetched 100 comments (total: 20199)\n",
            "Fetched 100 comments (total: 20299)\n",
            "Fetched 100 comments (total: 20399)\n",
            "Fetched 100 comments (total: 20499)\n",
            "Fetched 100 comments (total: 20599)\n",
            "Fetched 100 comments (total: 20699)\n",
            "Fetched 100 comments (total: 20799)\n",
            "Fetched 100 comments (total: 20899)\n",
            "Fetched 100 comments (total: 20999)\n",
            "Fetched 100 comments (total: 21099)\n",
            "Fetched 100 comments (total: 21199)\n",
            "Fetched 100 comments (total: 21299)\n",
            "Fetched 100 comments (total: 21399)\n",
            "Fetched 100 comments (total: 21499)\n",
            "Fetched 100 comments (total: 21599)\n",
            "Fetched 100 comments (total: 21699)\n",
            "Fetched 100 comments (total: 21799)\n",
            "Fetched 100 comments (total: 21899)\n",
            "Fetched 100 comments (total: 21999)\n",
            "Fetched 100 comments (total: 22099)\n",
            "Fetched 100 comments (total: 22199)\n",
            "Fetched 100 comments (total: 22299)\n",
            "Fetched 100 comments (total: 22399)\n",
            "Fetched 100 comments (total: 22499)\n",
            "Fetched 100 comments (total: 22599)\n",
            "Fetched 100 comments (total: 22699)\n",
            "Fetched 100 comments (total: 22799)\n",
            "Fetched 100 comments (total: 22899)\n",
            "Fetched 100 comments (total: 22999)\n",
            "Fetched 100 comments (total: 23099)\n",
            "Fetched 100 comments (total: 23199)\n",
            "Fetched 100 comments (total: 23299)\n",
            "Fetched 100 comments (total: 23399)\n",
            "Fetched 100 comments (total: 23499)\n",
            "Fetched 100 comments (total: 23599)\n",
            "Fetched 100 comments (total: 23699)\n",
            "Fetched 100 comments (total: 23799)\n",
            "Fetched 100 comments (total: 23899)\n",
            "Fetched 100 comments (total: 23999)\n",
            "Fetched 100 comments (total: 24099)\n",
            "Fetched 100 comments (total: 24199)\n",
            "Fetched 100 comments (total: 24299)\n",
            "Fetched 100 comments (total: 24399)\n",
            "Fetched 100 comments (total: 24499)\n",
            "Fetched 100 comments (total: 24599)\n",
            "Fetched 100 comments (total: 24699)\n",
            "Fetched 100 comments (total: 24799)\n",
            "Fetched 100 comments (total: 24899)\n",
            "Fetched 100 comments (total: 24999)\n",
            "Fetched 100 comments (total: 25099)\n",
            "Fetched 100 comments (total: 25199)\n",
            "Fetched 100 comments (total: 25299)\n",
            "Fetched 100 comments (total: 25399)\n",
            "Fetched 100 comments (total: 25499)\n",
            "Fetched 100 comments (total: 25599)\n",
            "Fetched 100 comments (total: 25699)\n",
            "Fetched 100 comments (total: 25799)\n",
            "Fetched 100 comments (total: 25899)\n",
            "Fetched 100 comments (total: 25999)\n",
            "Fetched 100 comments (total: 26099)\n",
            "Fetched 100 comments (total: 26199)\n",
            "Fetched 64 comments (total: 26263)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 13Apos39vHwqPe1-2WgJ-cETsLuzPmnPn\n",
            "Total comments saved: 26263\n",
            "Found 'comments.csv' with ID: 13Apos39vHwqPe1-2WgJ-cETsLuzPmnPn\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 15uwjPg5iJiTn18nsFywQ9lfq3SV7bveT\n",
            "Cleaned comments saved to: 1QR9Z19qT6Tj-B1N4bjzc1D-4dteE47k-/comments_preprocessed.csv\n",
            "Created new folder with ID: 1OBfJnHBBaawuR4kBPUBK0YftpfYS_Wk8\n",
            "Transcript loaded successfully into DataFrame.\n",
            "Uploaded 'transcript.csv' to Drive with ID: 1WnEKx8kywE7CMUF1-ypvZMWZeADhCA6E\n",
            "Video metadata loaded successfully into DataFrame.\n",
            "Uploaded 'video_metadata.csv' to Drive with ID: 1V6_HWl4aNbhCPdrZlddy5v38D36MYEEX\n",
            "Fetched 100 comments (total: 100)\n",
            "Fetched 100 comments (total: 200)\n",
            "Fetched 100 comments (total: 300)\n",
            "Fetched 100 comments (total: 400)\n",
            "Fetched 100 comments (total: 500)\n",
            "Fetched 100 comments (total: 600)\n",
            "Fetched 100 comments (total: 700)\n",
            "Fetched 100 comments (total: 800)\n",
            "Fetched 100 comments (total: 900)\n",
            "Fetched 100 comments (total: 1000)\n",
            "Fetched 100 comments (total: 1100)\n",
            "Fetched 100 comments (total: 1200)\n",
            "Fetched 100 comments (total: 1300)\n",
            "Fetched 100 comments (total: 1400)\n",
            "Fetched 100 comments (total: 1500)\n",
            "Fetched 100 comments (total: 1600)\n",
            "Fetched 100 comments (total: 1700)\n",
            "Fetched 100 comments (total: 1800)\n",
            "Fetched 100 comments (total: 1900)\n",
            "Fetched 100 comments (total: 2000)\n",
            "Fetched 100 comments (total: 2100)\n",
            "Fetched 100 comments (total: 2200)\n",
            "Fetched 100 comments (total: 2300)\n",
            "Fetched 100 comments (total: 2400)\n",
            "Fetched 100 comments (total: 2500)\n",
            "Fetched 100 comments (total: 2600)\n",
            "Fetched 100 comments (total: 2700)\n",
            "Fetched 100 comments (total: 2800)\n",
            "Fetched 100 comments (total: 2900)\n",
            "Fetched 100 comments (total: 3000)\n",
            "Fetched 100 comments (total: 3100)\n",
            "Fetched 100 comments (total: 3200)\n",
            "Fetched 100 comments (total: 3300)\n",
            "Fetched 100 comments (total: 3400)\n",
            "Fetched 100 comments (total: 3500)\n",
            "Fetched 100 comments (total: 3600)\n",
            "Fetched 100 comments (total: 3700)\n",
            "Fetched 100 comments (total: 3800)\n",
            "Fetched 100 comments (total: 3900)\n",
            "Fetched 100 comments (total: 4000)\n",
            "Fetched 100 comments (total: 4100)\n",
            "Fetched 100 comments (total: 4200)\n",
            "Fetched 100 comments (total: 4300)\n",
            "Fetched 100 comments (total: 4400)\n",
            "Fetched 100 comments (total: 4500)\n",
            "Fetched 100 comments (total: 4600)\n",
            "Fetched 100 comments (total: 4700)\n",
            "Fetched 100 comments (total: 4800)\n",
            "Fetched 100 comments (total: 4900)\n",
            "Fetched 100 comments (total: 5000)\n",
            "Fetched 100 comments (total: 5100)\n",
            "Fetched 100 comments (total: 5200)\n",
            "Fetched 100 comments (total: 5300)\n",
            "Fetched 100 comments (total: 5400)\n",
            "Fetched 100 comments (total: 5500)\n",
            "Fetched 100 comments (total: 5600)\n",
            "Fetched 100 comments (total: 5700)\n",
            "Fetched 100 comments (total: 5800)\n",
            "Fetched 100 comments (total: 5900)\n",
            "Fetched 100 comments (total: 6000)\n",
            "Fetched 100 comments (total: 6100)\n",
            "Fetched 100 comments (total: 6200)\n",
            "Fetched 100 comments (total: 6300)\n",
            "Fetched 100 comments (total: 6400)\n",
            "Fetched 100 comments (total: 6500)\n",
            "Fetched 100 comments (total: 6600)\n",
            "Fetched 100 comments (total: 6700)\n",
            "Fetched 100 comments (total: 6800)\n",
            "Fetched 100 comments (total: 6900)\n",
            "Fetched 100 comments (total: 7000)\n",
            "Fetched 100 comments (total: 7100)\n",
            "Fetched 100 comments (total: 7200)\n",
            "Fetched 100 comments (total: 7300)\n",
            "Fetched 100 comments (total: 7400)\n",
            "Fetched 100 comments (total: 7500)\n",
            "Fetched 100 comments (total: 7600)\n",
            "Fetched 100 comments (total: 7700)\n",
            "Fetched 100 comments (total: 7800)\n",
            "Fetched 100 comments (total: 7900)\n",
            "Fetched 100 comments (total: 8000)\n",
            "Fetched 100 comments (total: 8100)\n",
            "Fetched 100 comments (total: 8200)\n",
            "Fetched 100 comments (total: 8300)\n",
            "Fetched 100 comments (total: 8400)\n",
            "Fetched 100 comments (total: 8500)\n",
            "Fetched 100 comments (total: 8600)\n",
            "Fetched 100 comments (total: 8700)\n",
            "Fetched 100 comments (total: 8800)\n",
            "Fetched 100 comments (total: 8900)\n",
            "Fetched 100 comments (total: 9000)\n",
            "Fetched 100 comments (total: 9100)\n",
            "Fetched 100 comments (total: 9200)\n",
            "Fetched 100 comments (total: 9300)\n",
            "Fetched 100 comments (total: 9400)\n",
            "Fetched 100 comments (total: 9500)\n",
            "Fetched 100 comments (total: 9600)\n",
            "Fetched 100 comments (total: 9700)\n",
            "Fetched 100 comments (total: 9800)\n",
            "Fetched 100 comments (total: 9900)\n",
            "Fetched 100 comments (total: 10000)\n",
            "Fetched 100 comments (total: 10100)\n",
            "Fetched 100 comments (total: 10200)\n",
            "Fetched 100 comments (total: 10300)\n",
            "Fetched 100 comments (total: 10400)\n",
            "Fetched 100 comments (total: 10500)\n",
            "Fetched 100 comments (total: 10600)\n",
            "Fetched 100 comments (total: 10700)\n",
            "Fetched 100 comments (total: 10800)\n",
            "Fetched 100 comments (total: 10900)\n",
            "Fetched 100 comments (total: 11000)\n",
            "Fetched 100 comments (total: 11100)\n",
            "Fetched 100 comments (total: 11200)\n",
            "Fetched 100 comments (total: 11300)\n",
            "Fetched 100 comments (total: 11400)\n",
            "Fetched 100 comments (total: 11500)\n",
            "Fetched 100 comments (total: 11600)\n",
            "Fetched 100 comments (total: 11700)\n",
            "Fetched 100 comments (total: 11800)\n",
            "Fetched 100 comments (total: 11900)\n",
            "Fetched 100 comments (total: 12000)\n",
            "Fetched 100 comments (total: 12100)\n",
            "Fetched 100 comments (total: 12200)\n",
            "Fetched 100 comments (total: 12300)\n",
            "Fetched 100 comments (total: 12400)\n",
            "Fetched 100 comments (total: 12500)\n",
            "Fetched 100 comments (total: 12600)\n",
            "Fetched 100 comments (total: 12700)\n",
            "Fetched 100 comments (total: 12800)\n",
            "Fetched 100 comments (total: 12900)\n",
            "Fetched 100 comments (total: 13000)\n",
            "Fetched 100 comments (total: 13100)\n",
            "Fetched 100 comments (total: 13200)\n",
            "Fetched 100 comments (total: 13300)\n",
            "Fetched 100 comments (total: 13400)\n",
            "Fetched 100 comments (total: 13500)\n",
            "Fetched 100 comments (total: 13600)\n",
            "Fetched 100 comments (total: 13700)\n",
            "Fetched 100 comments (total: 13800)\n",
            "Fetched 100 comments (total: 13900)\n",
            "Fetched 100 comments (total: 14000)\n",
            "Fetched 100 comments (total: 14100)\n",
            "Fetched 100 comments (total: 14200)\n",
            "Fetched 100 comments (total: 14300)\n",
            "Fetched 100 comments (total: 14400)\n",
            "Fetched 100 comments (total: 14500)\n",
            "Fetched 100 comments (total: 14600)\n",
            "Fetched 100 comments (total: 14700)\n",
            "Fetched 100 comments (total: 14800)\n",
            "Fetched 100 comments (total: 14900)\n",
            "Fetched 100 comments (total: 15000)\n",
            "Fetched 100 comments (total: 15100)\n",
            "Fetched 100 comments (total: 15200)\n",
            "Fetched 100 comments (total: 15300)\n",
            "Fetched 100 comments (total: 15400)\n",
            "Fetched 100 comments (total: 15500)\n",
            "Fetched 100 comments (total: 15600)\n",
            "Fetched 100 comments (total: 15700)\n",
            "Fetched 100 comments (total: 15800)\n",
            "Fetched 29 comments (total: 15829)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 1HxxtyVhEoKPAAupzjhSJFgs92GfnCRhw\n",
            "Total comments saved: 15829\n",
            "Found 'comments.csv' with ID: 1HxxtyVhEoKPAAupzjhSJFgs92GfnCRhw\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 1d76RK2a23HgNVX9VAjok9LFlc-CR_6W4\n",
            "Cleaned comments saved to: 1OBfJnHBBaawuR4kBPUBK0YftpfYS_Wk8/comments_preprocessed.csv\n",
            "Created new folder with ID: 1MBlveZsnEeSYgPBvItFNvh1CXo55dQnp\n",
            "Transcript loaded successfully into DataFrame.\n",
            "Uploaded 'transcript.csv' to Drive with ID: 1DqOYN6wFQPHTmxyVO7b82P4A-gvOITp1\n",
            "Video metadata loaded successfully into DataFrame.\n",
            "Uploaded 'video_metadata.csv' to Drive with ID: 1Qi3UwAMUhbaIcQ8f873NUdQanAL7fks0\n",
            "Fetched 100 comments (total: 100)\n",
            "Fetched 100 comments (total: 200)\n",
            "Fetched 100 comments (total: 300)\n",
            "Fetched 100 comments (total: 400)\n",
            "Fetched 100 comments (total: 500)\n",
            "Fetched 100 comments (total: 600)\n",
            "Fetched 100 comments (total: 700)\n",
            "Fetched 100 comments (total: 800)\n",
            "Fetched 100 comments (total: 900)\n",
            "Fetched 100 comments (total: 1000)\n",
            "Fetched 100 comments (total: 1100)\n",
            "Fetched 100 comments (total: 1200)\n",
            "Fetched 100 comments (total: 1300)\n",
            "Fetched 100 comments (total: 1400)\n",
            "Fetched 100 comments (total: 1500)\n",
            "Fetched 100 comments (total: 1600)\n",
            "Fetched 100 comments (total: 1700)\n",
            "Fetched 100 comments (total: 1800)\n",
            "Fetched 100 comments (total: 1900)\n",
            "Fetched 100 comments (total: 2000)\n",
            "Fetched 100 comments (total: 2100)\n",
            "Fetched 100 comments (total: 2200)\n",
            "Fetched 100 comments (total: 2300)\n",
            "Fetched 100 comments (total: 2400)\n",
            "Fetched 100 comments (total: 2500)\n",
            "Fetched 100 comments (total: 2600)\n",
            "Fetched 100 comments (total: 2700)\n",
            "Fetched 100 comments (total: 2800)\n",
            "Fetched 100 comments (total: 2900)\n",
            "Fetched 100 comments (total: 3000)\n",
            "Fetched 100 comments (total: 3100)\n",
            "Fetched 100 comments (total: 3200)\n",
            "Fetched 100 comments (total: 3300)\n",
            "Fetched 100 comments (total: 3400)\n",
            "Fetched 100 comments (total: 3500)\n",
            "Fetched 100 comments (total: 3600)\n",
            "Fetched 100 comments (total: 3700)\n",
            "Fetched 100 comments (total: 3800)\n",
            "Fetched 100 comments (total: 3900)\n",
            "Fetched 100 comments (total: 4000)\n",
            "Fetched 100 comments (total: 4100)\n",
            "Fetched 100 comments (total: 4200)\n",
            "Fetched 100 comments (total: 4300)\n",
            "Fetched 100 comments (total: 4400)\n",
            "Fetched 100 comments (total: 4500)\n",
            "Fetched 100 comments (total: 4600)\n",
            "Fetched 100 comments (total: 4700)\n",
            "Fetched 100 comments (total: 4800)\n",
            "Fetched 100 comments (total: 4900)\n",
            "Fetched 100 comments (total: 5000)\n",
            "Fetched 100 comments (total: 5100)\n",
            "Fetched 100 comments (total: 5200)\n",
            "Fetched 100 comments (total: 5300)\n",
            "Fetched 100 comments (total: 5400)\n",
            "Fetched 100 comments (total: 5500)\n",
            "Fetched 100 comments (total: 5600)\n",
            "Fetched 100 comments (total: 5700)\n",
            "Fetched 100 comments (total: 5800)\n",
            "Fetched 100 comments (total: 5900)\n",
            "Fetched 100 comments (total: 6000)\n",
            "Fetched 100 comments (total: 6100)\n",
            "Fetched 100 comments (total: 6200)\n",
            "Fetched 100 comments (total: 6300)\n",
            "Fetched 100 comments (total: 6400)\n",
            "Fetched 100 comments (total: 6500)\n",
            "Fetched 100 comments (total: 6600)\n",
            "Fetched 100 comments (total: 6700)\n",
            "Fetched 100 comments (total: 6800)\n",
            "Fetched 100 comments (total: 6900)\n",
            "Fetched 100 comments (total: 7000)\n",
            "Fetched 100 comments (total: 7100)\n",
            "Fetched 100 comments (total: 7200)\n",
            "Fetched 100 comments (total: 7300)\n",
            "Fetched 100 comments (total: 7400)\n",
            "Fetched 100 comments (total: 7500)\n",
            "Fetched 100 comments (total: 7600)\n",
            "Fetched 79 comments (total: 7679)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 143WJm0XCgg2dONGOJk1MMTxCVYnW9h_z\n",
            "Total comments saved: 7679\n",
            "Found 'comments.csv' with ID: 143WJm0XCgg2dONGOJk1MMTxCVYnW9h_z\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 1owiAjA-ksvpKzRHu5sx0C7x2bB5tz2zv\n",
            "Cleaned comments saved to: 1MBlveZsnEeSYgPBvItFNvh1CXo55dQnp/comments_preprocessed.csv\n",
            "Using existing folder with ID: 1PQfZrne9AaGX8XvSWpsmOl0EG3yBbmf7\n",
            "Transcript loaded successfully into DataFrame.\n",
            "Uploaded 'transcript.csv' to Drive with ID: 1f9Bfi_einhQXJJ35_7FV5ldr_0RC92W0\n",
            "Video metadata loaded successfully into DataFrame.\n",
            "Uploaded 'video_metadata.csv' to Drive with ID: 1_t6FSj8xK3Tke-wke5FLcRHEP1L4783v\n",
            "Fetched 100 comments (total: 100)\n",
            "Fetched 100 comments (total: 200)\n",
            "Fetched 100 comments (total: 300)\n",
            "Fetched 100 comments (total: 400)\n",
            "Fetched 100 comments (total: 500)\n",
            "Fetched 100 comments (total: 600)\n",
            "Fetched 100 comments (total: 700)\n",
            "Fetched 100 comments (total: 800)\n",
            "Fetched 100 comments (total: 900)\n",
            "Fetched 100 comments (total: 1000)\n",
            "Fetched 100 comments (total: 1100)\n",
            "Fetched 100 comments (total: 1200)\n",
            "Fetched 100 comments (total: 1300)\n",
            "Fetched 86 comments (total: 1386)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 1CKODdfV_I78gG5rzWVluLDAWhiWveLbx\n",
            "Total comments saved: 1386\n",
            "Found 'comments.csv' with ID: 1CKODdfV_I78gG5rzWVluLDAWhiWveLbx\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 1lnoJK7KzKMyU5gyd5dIALMri_qvWhYyd\n",
            "Cleaned comments saved to: 1PQfZrne9AaGX8XvSWpsmOl0EG3yBbmf7/comments_preprocessed.csv\n",
            "Created new folder with ID: 1zqbg95zeUXfo7PN-20a1FsVRj6CW3mpK\n",
            "No English transcript found for video ID: fqyl5kbZ7Tw\n",
            "Video metadata loaded successfully into DataFrame.\n",
            "Uploaded 'video_metadata.csv' to Drive with ID: 1DcsFWwJ6BEMvRA6kWeyDVrUeszRUcays\n",
            "Fetched 99 comments (total: 99)\n",
            "Fetched 100 comments (total: 199)\n",
            "Fetched 100 comments (total: 299)\n",
            "Fetched 100 comments (total: 399)\n",
            "Fetched 100 comments (total: 499)\n",
            "Fetched 100 comments (total: 599)\n",
            "Fetched 100 comments (total: 699)\n",
            "Fetched 100 comments (total: 799)\n",
            "Fetched 100 comments (total: 899)\n",
            "Fetched 100 comments (total: 999)\n",
            "Fetched 100 comments (total: 1099)\n",
            "Fetched 100 comments (total: 1199)\n",
            "Fetched 100 comments (total: 1299)\n",
            "Fetched 100 comments (total: 1399)\n",
            "Fetched 100 comments (total: 1499)\n",
            "Fetched 100 comments (total: 1599)\n",
            "Fetched 100 comments (total: 1699)\n",
            "Fetched 100 comments (total: 1799)\n",
            "Fetched 100 comments (total: 1899)\n",
            "Fetched 100 comments (total: 1999)\n",
            "Fetched 100 comments (total: 2099)\n",
            "Fetched 100 comments (total: 2199)\n",
            "Fetched 100 comments (total: 2299)\n",
            "Fetched 100 comments (total: 2399)\n",
            "Fetched 100 comments (total: 2499)\n",
            "Fetched 100 comments (total: 2599)\n",
            "Fetched 100 comments (total: 2699)\n",
            "Fetched 67 comments (total: 2766)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 12i-DpsMN-sfmAo97omJl9_JOkED64ima\n",
            "Total comments saved: 2766\n",
            "Found 'comments.csv' with ID: 12i-DpsMN-sfmAo97omJl9_JOkED64ima\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 103kSKRgv8MUphtG_XV7Q8tKBHgMd4sPJ\n",
            "Cleaned comments saved to: 1zqbg95zeUXfo7PN-20a1FsVRj6CW3mpK/comments_preprocessed.csv\n",
            "Created new folder with ID: 1OfGZm-FI7N5BKUxSnpzYG-_Haw9F_M0L\n",
            "Transcript loaded successfully into DataFrame.\n",
            "Uploaded 'transcript.csv' to Drive with ID: 1sHgoffZQAzlrxImArc0IoqIWSEwmI0aD\n",
            "Video metadata loaded successfully into DataFrame.\n",
            "Uploaded 'video_metadata.csv' to Drive with ID: 1GO5hhDzaSHedRGxmgGYrZ8fNx-cZmhCS\n",
            "Fetched 100 comments (total: 100)\n",
            "Fetched 100 comments (total: 200)\n",
            "Fetched 100 comments (total: 300)\n",
            "Fetched 100 comments (total: 400)\n",
            "Fetched 100 comments (total: 500)\n",
            "Fetched 100 comments (total: 600)\n",
            "Fetched 100 comments (total: 700)\n",
            "Fetched 100 comments (total: 800)\n",
            "Fetched 100 comments (total: 900)\n",
            "Fetched 100 comments (total: 1000)\n",
            "Fetched 100 comments (total: 1100)\n",
            "Fetched 100 comments (total: 1200)\n",
            "Fetched 100 comments (total: 1300)\n",
            "Fetched 100 comments (total: 1400)\n",
            "Fetched 100 comments (total: 1500)\n",
            "Fetched 100 comments (total: 1600)\n",
            "Fetched 100 comments (total: 1700)\n",
            "Fetched 100 comments (total: 1800)\n",
            "Fetched 100 comments (total: 1900)\n",
            "Fetched 100 comments (total: 2000)\n",
            "Fetched 100 comments (total: 2100)\n",
            "Fetched 100 comments (total: 2200)\n",
            "Fetched 100 comments (total: 2300)\n",
            "Fetched 100 comments (total: 2400)\n",
            "Fetched 100 comments (total: 2500)\n",
            "Fetched 100 comments (total: 2600)\n",
            "Fetched 100 comments (total: 2700)\n",
            "Fetched 100 comments (total: 2800)\n",
            "Fetched 100 comments (total: 2900)\n",
            "Fetched 100 comments (total: 3000)\n",
            "Fetched 100 comments (total: 3100)\n",
            "Fetched 100 comments (total: 3200)\n",
            "Fetched 100 comments (total: 3300)\n",
            "Fetched 100 comments (total: 3400)\n",
            "Fetched 100 comments (total: 3500)\n",
            "Fetched 100 comments (total: 3600)\n",
            "Fetched 100 comments (total: 3700)\n",
            "Fetched 100 comments (total: 3800)\n",
            "Fetched 100 comments (total: 3900)\n",
            "Fetched 100 comments (total: 4000)\n",
            "Fetched 71 comments (total: 4071)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 1pQJh4_qPrqK5AXLifr7dv_VtvTYxPWF4\n",
            "Total comments saved: 4071\n",
            "Found 'comments.csv' with ID: 1pQJh4_qPrqK5AXLifr7dv_VtvTYxPWF4\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 14V0WPssQGShp0N9QLX74uKcyf6qStaON\n",
            "Cleaned comments saved to: 1OfGZm-FI7N5BKUxSnpzYG-_Haw9F_M0L/comments_preprocessed.csv\n",
            "Created new folder with ID: 1tCCeq0WJ75cfX0jmvL7MQJiFmsmcZ3V5\n",
            "Transcript loaded successfully into DataFrame.\n",
            "Uploaded 'transcript.csv' to Drive with ID: 1E_HLFu0i2q0jX3NnXdENR5ligqzkNL1f\n",
            "Video metadata loaded successfully into DataFrame.\n",
            "Uploaded 'video_metadata.csv' to Drive with ID: 12aMLS55_KMXDKfSyy6Qt5vHB0Be4PyCf\n",
            "Fetched 100 comments (total: 100)\n",
            "Fetched 100 comments (total: 200)\n",
            "Fetched 100 comments (total: 300)\n",
            "Fetched 100 comments (total: 400)\n",
            "Fetched 100 comments (total: 500)\n",
            "Fetched 100 comments (total: 600)\n",
            "Fetched 100 comments (total: 700)\n",
            "Fetched 100 comments (total: 800)\n",
            "Fetched 100 comments (total: 900)\n",
            "Fetched 100 comments (total: 1000)\n",
            "Fetched 100 comments (total: 1100)\n",
            "Fetched 100 comments (total: 1200)\n",
            "Fetched 100 comments (total: 1300)\n",
            "Fetched 100 comments (total: 1400)\n",
            "Fetched 100 comments (total: 1500)\n",
            "Fetched 100 comments (total: 1600)\n",
            "Fetched 100 comments (total: 1700)\n",
            "Fetched 100 comments (total: 1800)\n",
            "Fetched 100 comments (total: 1900)\n",
            "Fetched 100 comments (total: 2000)\n",
            "Fetched 100 comments (total: 2100)\n",
            "Fetched 100 comments (total: 2200)\n",
            "Fetched 100 comments (total: 2300)\n",
            "Fetched 100 comments (total: 2400)\n",
            "Fetched 100 comments (total: 2500)\n",
            "Fetched 100 comments (total: 2600)\n",
            "Fetched 100 comments (total: 2700)\n",
            "Fetched 100 comments (total: 2800)\n",
            "Fetched 100 comments (total: 2900)\n",
            "Fetched 100 comments (total: 3000)\n",
            "Fetched 100 comments (total: 3100)\n",
            "Fetched 100 comments (total: 3200)\n",
            "Fetched 100 comments (total: 3300)\n",
            "Fetched 100 comments (total: 3400)\n",
            "Fetched 100 comments (total: 3500)\n",
            "Fetched 100 comments (total: 3600)\n",
            "Fetched 100 comments (total: 3700)\n",
            "Fetched 100 comments (total: 3800)\n",
            "Fetched 100 comments (total: 3900)\n",
            "Fetched 100 comments (total: 4000)\n",
            "Fetched 100 comments (total: 4100)\n",
            "Fetched 100 comments (total: 4200)\n",
            "Fetched 100 comments (total: 4300)\n",
            "Fetched 100 comments (total: 4400)\n",
            "Fetched 100 comments (total: 4500)\n",
            "Fetched 100 comments (total: 4600)\n",
            "Fetched 100 comments (total: 4700)\n",
            "Fetched 100 comments (total: 4800)\n",
            "Fetched 100 comments (total: 4900)\n",
            "Fetched 100 comments (total: 5000)\n",
            "Fetched 100 comments (total: 5100)\n",
            "Fetched 100 comments (total: 5200)\n",
            "Fetched 100 comments (total: 5300)\n",
            "Fetched 100 comments (total: 5400)\n",
            "Fetched 100 comments (total: 5500)\n",
            "Fetched 100 comments (total: 5600)\n",
            "Fetched 100 comments (total: 5700)\n",
            "Fetched 100 comments (total: 5800)\n",
            "Fetched 100 comments (total: 5900)\n",
            "Fetched 100 comments (total: 6000)\n",
            "Fetched 100 comments (total: 6100)\n",
            "Fetched 100 comments (total: 6200)\n",
            "Fetched 100 comments (total: 6300)\n",
            "Fetched 100 comments (total: 6400)\n",
            "Fetched 100 comments (total: 6500)\n",
            "Fetched 100 comments (total: 6600)\n",
            "Fetched 100 comments (total: 6700)\n",
            "Fetched 100 comments (total: 6800)\n",
            "Fetched 18 comments (total: 6818)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 1tw-R3J8GOfS3C0CNwKRPekC531cDJjpT\n",
            "Total comments saved: 6818\n",
            "Found 'comments.csv' with ID: 1tw-R3J8GOfS3C0CNwKRPekC531cDJjpT\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 12GUx6OfGwUoT-lgc2ijTJivA0UeQqHD6\n",
            "Cleaned comments saved to: 1tCCeq0WJ75cfX0jmvL7MQJiFmsmcZ3V5/comments_preprocessed.csv\n"
          ]
        }
      ],
      "source": [
        "VIDEO_ID_LIST = [\n",
        "    'teMKnxbd_O0',\n",
        "    'PssKpzB0Ah0',\n",
        "    'UiIRlg4Xr5w',\n",
        "    'YYQXk1t_JHM',\n",
        "    '7qriveAV7BY',\n",
        "    'fqyl5kbZ7Tw',\n",
        "    '7ARBJQn6QkM',\n",
        "    'hmtuvNfytjM'\n",
        "]\n",
        "\n",
        "for YOUTUBE_VIDEO_ID in VIDEO_ID_LIST:\n",
        "    # Drive setup\n",
        "    VIDEO_FOLDER_ID = setup_drive(YOUTUBE_VIDEO_ID, DRIVE_FOLDER_ID)\n",
        "    # Transcript extraction\n",
        "    transcript_data = fetch_english_transcript(YOUTUBE_VIDEO_ID)\n",
        "    if transcript_data:\n",
        "        df_transcript = pd.DataFrame(transcript_data)\n",
        "        print(\"Transcript loaded successfully into DataFrame.\")\n",
        "\n",
        "        # Upload to drive using the existing df_to_drive function\n",
        "        df_to_drive(df_transcript, VIDEO_FOLDER_ID, 'transcript.csv')\n",
        "    else:\n",
        "        print(f\"No English transcript found for video ID: {YOUTUBE_VIDEO_ID}\")\n",
        "    # Metadata extraction\n",
        "    df_metadata = get_video_metadata(YOUTUBE_VIDEO_ID)\n",
        "\n",
        "    if not df_metadata.empty:\n",
        "        print(\"Video metadata loaded successfully into DataFrame.\")\n",
        "        # Upload to drive using the existing df_to_drive function\n",
        "        df_to_drive(df_metadata, VIDEO_FOLDER_ID, 'video_metadata.csv')\n",
        "    else:\n",
        "        print(f\"Failed to retrieve metadata for video ID: {YOUTUBE_VIDEO_ID}\")\n",
        "    # Comments Extraction\n",
        "    save_youtube_comments_to_drive(\n",
        "        YOUTUBE_VIDEO_ID,\n",
        "        YOUTUBE_API_KEY,\n",
        "        service,\n",
        "        VIDEO_FOLDER_ID\n",
        "    )\n",
        "    # Preprocess comments\n",
        "    preprocess_comments(VIDEO_FOLDER_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3D9flZA2ViA"
      },
      "source": [
        "Multilingual Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1WW807s2ZVE"
      },
      "outputs": [],
      "source": [
        "#install dependencies\n",
        "\n",
        "!pip install googletrans==4.0.0-rc1 langdetect\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "from langdetect import detect, LangDetectException\n",
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "# Drop rows with empty or invalid cleaned_comment\n",
        "df = df[df['cleaned_comment'].str.strip().str.len() > 3].copy()\n",
        "\n",
        "# Function to safely detect language\n",
        "def safe_detect(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        return \"unknown\"\n",
        "\n",
        "# Apply language detection\n",
        "df['language'] = df['cleaned_comment'].apply(safe_detect)\n",
        "\n",
        "# Translate if not English\n",
        "def translate_to_english(row):\n",
        "    if row['language'] == 'en':\n",
        "        return row['cleaned_comment']\n",
        "    try:\n",
        "        return translator.translate(row['cleaned_comment'], dest='en').text\n",
        "    except Exception:\n",
        "        return row['cleaned_comment']  # fallback to original if translation fails\n",
        "\n",
        "df['translated_comment'] = df.apply(translate_to_english, axis=1)\n",
        "\n",
        "# Optional: save to Drive\n",
        "df.to_csv(\"/content/drive/MyDrive/comments_translated.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Translation complete. File saved to Drive.\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvLr0hd0qapz"
      },
      "source": [
        "### Intent Classification (Praise, Question, Criticism, or Suggestion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEk4gwLVycNL"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHposg_XEq45"
      },
      "source": [
        "### Sentiment Classfication (Positive, Negative, Neutral)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wJ6Qr4ifrX1m",
        "outputId": "591bfbda-c1e8-4de9-b3e4-7081dad8094d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "#install dependences\n",
        "!pip install transformers torch\n",
        "#load cleaned data\n",
        "import pandas as pd\n",
        "#for graph visulaization\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCNwSMUaU5Um"
      },
      "source": [
        "load sentiment piple line -pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MJ9YGVd5TZkU",
        "outputId": "ac3d5f7f-120e-4a03-ea23-6a0f3edf3622"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Sentiment-labeled file saved to Drive.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Update the path based on where your file is\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/youtube_comments.csv\")\n",
        "\n",
        "# Drop missing or empty comments just in case\n",
        "df = df.dropna(subset=[\"cleaned_comment\"])\n",
        "df = df[df[\"cleaned_comment\"].str.strip() != \"\"]\n",
        "#load pretrained model\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline for sentiment analysis\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "#run anaylsis on my own data(test-data)\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        # Truncate input text safely to 512 tokens max (roberta's limit)\n",
        "        encoded = tokenizer(text, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "        output = model(**encoded)\n",
        "        prediction = output.logits.argmax(dim=1).item()\n",
        "\n",
        "        label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "        return label_map[prediction]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing: {text[:50]}... â€” {str(e)}\")\n",
        "        return \"Unknown\"\n",
        "# You can test on a subset first\n",
        "df = df.head(100)\n",
        "\n",
        "df['sentiment'] = df['cleaned_comment'].apply(get_sentiment)\n",
        "#df.to_csv(\"/content/drive/MyDrive/comments_with_sentiment.csv\", index=False)\n",
        "print(\" Sentiment-labeled file saved to Drive.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq2KI6zzVofL"
      },
      "source": [
        "**visulaize output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mgCD8AucpKn"
      },
      "outputs": [],
      "source": [
        "df['sentiment'].value_counts().plot.pie(autopct='%1.1f%%', colors=[\"red\", \"gray\", \"green\"], title=\"Sentiment Distribution\")\n",
        "plt.ylabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tRfVzvWU7W5"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WZ__STY1Eueb",
        "outputId": "bd6c0ee3-cbc1-4e91-d0de-cbf7a09de89c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'so'"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8ZYlX5eEjGA"
      },
      "source": [
        "### Topic Classification (BERTopic)\n",
        "\n",
        "1.   *List item*\n",
        "2.   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA0EAHY1EpIG"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoBZ90AlEyAT"
      },
      "source": [
        "### Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-uV_metE00E"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUPbOJCbBi82"
      },
      "source": [
        "### Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PRrgmNYcbNaS",
        "outputId": "d2c37b23-a6cc-4c99-f9f6-85e041d01b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-6.0.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Collecting gradio-client==2.0.1 (from gradio)\n",
            "  Downloading gradio_client-2.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<13.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.4,>=2.11.10 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==2.0.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.4,>=2.11.10->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-6.0.2-py3-none-any.whl (21.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-2.0.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gradio-client, gradio\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.14.0\n",
            "    Uninstalling gradio_client-1.14.0:\n",
            "      Successfully uninstalled gradio_client-1.14.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.50.0\n",
            "    Uninstalling gradio-5.50.0:\n",
            "      Successfully uninstalled gradio-5.50.0\n",
            "Successfully installed gradio-6.0.2 gradio-client-2.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gradio"
                ]
              },
              "id": "1ddf2d161f5345f6be2812f1c73d5169"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "k1qecEi1BipI",
        "outputId": "f98defc0-dcb5-40d0-ab25-46cd06a254ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0.2\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://2a39428e16ea785c1b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2a39428e16ea785c1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created new folder with ID: 1lGT009GqIBU0TN-M0-XcVszk-RHFW_JM\n",
            "Fetched 83 comments (total: 83)\n",
            "Uploading to Google Drive...\n",
            "Uploaded 'comments.csv' to Drive with ID: 1fXEZ374_wJjDy0Ub2hf3qvbhPETDmPtb\n",
            "Total comments saved: 83\n",
            "Found 'comments.csv' with ID: 1fXEZ374_wJjDy0Ub2hf3qvbhPETDmPtb\n",
            "Download 100%.\n",
            "Comments loaded successfully into df.\n",
            "Uploaded 'comments_preprocessed.csv' to Drive with ID: 1SvoXshXDMOgjpr0dMjVOYe1gQo7ehILi\n",
            "Cleaned comments saved to: 1lGT009GqIBU0TN-M0-XcVszk-RHFW_JM/comments_preprocessed.csv\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "print(gr.__version__)\n",
        "import re\n",
        "\n",
        "# --- Your helpers ---\n",
        "\n",
        "def extract_video_id(url):\n",
        "    pattern = r\"(?:v=|youtu\\.be/)([a-zA-Z0-9_-]{11})\"\n",
        "    match = re.search(pattern, url)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# UI update helpers\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "def metadata_update(meta):\n",
        "    \"\"\"\n",
        "    Returns UI update tuple for metadata components\n",
        "    \"\"\"\n",
        "    def truncate(description):\n",
        "        short_desc = \"\\n\".join(description.split(\"\\n\")[:4] + ['[TRUNCATED]'])  # first 4 lines only\n",
        "        return short_desc\n",
        "\n",
        "    return (\n",
        "        gr.update(value=meta[\"thumb\"], visible=meta[\"thumb\"] is not None),\n",
        "        gr.update(value=meta[\"title\"], visible=bool(meta[\"title\"])),\n",
        "        gr.update(value=meta[\"published\"], visible=bool(meta[\"published\"])),\n",
        "        gr.update(value=truncate(meta[\"description\"]), visible=bool(meta[\"description\"])),\n",
        "        gr.update(value=meta[\"category\"], visible=bool(meta[\"category\"])),\n",
        "    )\n",
        "\n",
        "\n",
        "def transcript_update(transcript_df):\n",
        "    \"\"\"\n",
        "    Build transcript text output with formatted timestamps\n",
        "    and return UI update tuple.\n",
        "    \"\"\"\n",
        "\n",
        "    if transcript_df is None or transcript_df.empty:\n",
        "        return (gr.update(value=\"\", visible=False),)\n",
        "\n",
        "    # Format timestamp + text lines\n",
        "    formatted_lines = []\n",
        "    for _, row in transcript_df.iterrows():\n",
        "        minutes = int(row[\"start\"] // 60)\n",
        "        seconds = int(row[\"start\"] % 60)\n",
        "        ts = f\"{minutes:02d}:{seconds:02d}\"\n",
        "        formatted_lines.append(f\"[{ts}] {row['text']}\")\n",
        "\n",
        "    formatted_transcript = \"\\n\".join(formatted_lines)\n",
        "\n",
        "    return (gr.update(value=formatted_transcript, visible=True),)\n",
        "\n",
        "def comment_count_update(comments_df):\n",
        "    if comments_df is None or comments_df.empty:\n",
        "        return (gr.update(value=\"\", visible=False),)\n",
        "    return (gr.update(value=f\"Total comments: **{len(comments_df)}**\", visible=True),)\n",
        "\n",
        "\n",
        "def metadata_panel_state(meta):\n",
        "    \"\"\"\n",
        "    Makes metadata accordion visible only when metadata exists\n",
        "    \"\"\"\n",
        "    has_data = any([\n",
        "        meta[\"thumb\"],\n",
        "        meta[\"title\"],\n",
        "        meta[\"published\"],\n",
        "        meta[\"description\"],\n",
        "        meta[\"category\"]\n",
        "    ])\n",
        "    return gr.update(visible=has_data)\n",
        "\n",
        "\n",
        "def transcript_panel_state(transcript_df):\n",
        "    return gr.update(visible=bool(transcript_df is not None and not transcript_df.empty))\n",
        "\n",
        "\n",
        "def comment_count_panel_state(comments_df):\n",
        "    return gr.update(visible=bool(comments_df is not None and not comments_df.empty))\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Main pipeline\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "def process_video_url(url):\n",
        "    log = \"\"\n",
        "\n",
        "    # metadata placeholder container\n",
        "    meta = {\n",
        "        \"thumb\": None,\n",
        "        \"title\": \"\",\n",
        "        \"published\": \"\",\n",
        "        \"description\": \"\",\n",
        "        \"category\": \"\"\n",
        "    }\n",
        "\n",
        "    df_transcript = None\n",
        "    comments_df = None\n",
        "\n",
        "    # wrapper to emit current UI state\n",
        "    def emit():\n",
        "        return (\n",
        "            log,\n",
        "            metadata_panel_state(meta),\n",
        "            *metadata_update(meta),\n",
        "            transcript_panel_state(df_transcript),\n",
        "            *transcript_update(df_transcript),\n",
        "            comment_count_panel_state(comments_df),\n",
        "            *comment_count_update(comments_df)\n",
        "        )\n",
        "\n",
        "    # --- Pipeline execution ---\n",
        "\n",
        "    log += \"ðŸ” Validating YouTube URL...\\n\"\n",
        "    yield emit()\n",
        "\n",
        "    video_id = extract_video_id(url)\n",
        "    if not video_id:\n",
        "        log += \"âŒ Invalid YouTube URL.\\n\"\n",
        "        yield emit()\n",
        "        return\n",
        "\n",
        "    log += f\"âœ… Video ID extracted: {video_id}\\n\"\n",
        "    yield emit()\n",
        "\n",
        "    log += \"ðŸ“‚ Creating Google Drive folder...\\n\"\n",
        "    yield emit()\n",
        "\n",
        "    # Your drive setup call here\n",
        "    folder_id = setup_drive(video_id, DRIVE_FOLDER_ID)\n",
        "\n",
        "    log += f\"ðŸ“ Folder created: {folder_id}\\n\"\n",
        "    yield emit()\n",
        "\n",
        "    # Fetch transcript\n",
        "    log += \"ðŸ“ Fetching transcript...\\n\"\n",
        "    yield emit()\n",
        "    transcript = fetch_english_transcript(video_id)\n",
        "\n",
        "    if transcript:\n",
        "        df_transcript = pd.DataFrame(transcript)\n",
        "        log += \"âœ” Transcript fetched successfully.\\n\"\n",
        "    else:\n",
        "        df_transcript = None\n",
        "        log += \"âŒ Transcript unavailable.\\n\"\n",
        "    yield emit()\n",
        "\n",
        "    # Fetch metadata\n",
        "    log += \"ðŸ“Š Fetching metadata...\\n\"\n",
        "    yield emit()\n",
        "    df_metadata = get_video_metadata(video_id)\n",
        "\n",
        "    if not df_metadata.empty:\n",
        "        row = df_metadata.iloc[0]\n",
        "        meta[\"thumb\"]       = row[\"thumbnail_url\"]\n",
        "        meta[\"title\"]       = f\"**{row['title']}**\"\n",
        "        meta[\"published\"]   = f\"**Published:** {row['publishedAt']}\"\n",
        "        meta[\"description\"] = f\"**Description:**\\n{row['description']}\"\n",
        "        meta[\"category\"]    = f\"**Category ID:** {row['categoryId']}\"\n",
        "\n",
        "        log += \"âœ” Metadata loaded successfully.\\n\"\n",
        "    else:\n",
        "        log += \"âŒ Failed to load metadata.\\n\"\n",
        "    yield emit()\n",
        "\n",
        "    # Fetch comments\n",
        "    log += \"\\nðŸ“¥ Fetching comments...\\n\"\n",
        "    yield emit()\n",
        "    comments_df = save_youtube_comments_to_drive(video_id, YOUTUBE_API_KEY, service, folder_id)\n",
        "    log += f\"âœ… {len(comments_df)} comments fetched and saved to Drive.\\n\"\n",
        "    yield emit()\n",
        "\n",
        "    # Preprocess comments\n",
        "    log += \"ðŸ§¹ Preprocessing comments...\\n\"\n",
        "    yield emit()\n",
        "    comments_df = preprocess_comments(folder_id)\n",
        "    log += \"âœ… Comments preprocessed.\\n\"\n",
        "    yield emit()\n",
        "    return\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# GRADIO UI\n",
        "# --------------------------------------------------------------------\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ðŸŽ¥ YouTube Comments Insight Tool\")\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"Enter a YouTube URL â€” system will create Drive folder, fetch transcript, \"\n",
        "        \"and extract metadata. Future insights coming soon.\"\n",
        "    )\n",
        "\n",
        "    # URL entry\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            pass\n",
        "        with gr.Column(scale=2):\n",
        "            url_input = gr.Textbox(\n",
        "                label=\"Video URL\",\n",
        "                placeholder=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n",
        "            )\n",
        "            analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
        "        with gr.Column(scale=1):\n",
        "            pass\n",
        "\n",
        "    # logs panel\n",
        "    with gr.Accordion(\"ðŸ“œ Logs\", open=True):\n",
        "        status_box = gr.Textbox(\n",
        "            label=\"Execution Log\",\n",
        "            max_lines=10,\n",
        "            autoscroll=True,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "    # --- metadata accordion (hidden initially) ---\n",
        "    metadata_accordion = gr.Accordion(\"ðŸ“¼ Video Metadata\", open=False, visible=False)\n",
        "\n",
        "    with metadata_accordion:\n",
        "        with gr.Row():\n",
        "            thumbnail = gr.Image(label=\"Thumbnail\", visible=False, height=180)\n",
        "\n",
        "            with gr.Column():\n",
        "                title_md = gr.Markdown(visible=False)\n",
        "                published_md = gr.Markdown(visible=False)\n",
        "                category_md = gr.Markdown(visible=False)\n",
        "\n",
        "        description_md = gr.Markdown(visible=False)\n",
        "\n",
        "    # --- full transcript accordion (hidden initially) ---\n",
        "    transcript_accordion = gr.Accordion(\"ðŸ“ Transcript\", open=False, visible=False)\n",
        "\n",
        "    with transcript_accordion:\n",
        "        transcript_md = gr.Textbox(\n",
        "            label=\"Transcript\",\n",
        "            container=False,\n",
        "            lines=20,\n",
        "            interactive=False\n",
        "        )\n",
        "\n",
        "    # --- Comments Count panel (hidden initially) ---\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            comment_count_accordion = gr.Accordion(\"ðŸ’¬ Comments\", open=True, visible=False)\n",
        "            with comment_count_accordion:\n",
        "                comment_count_md = gr.Markdown(visible=False)\n",
        "        with gr.Column(scale=2):\n",
        "            pass\n",
        "\n",
        "    # bind outputs\n",
        "    analyze_btn.click(\n",
        "        fn=process_video_url,\n",
        "        inputs=url_input,\n",
        "        outputs=[\n",
        "            status_box,          # log text\n",
        "            metadata_accordion,  # show/hide metadata accordion\n",
        "            thumbnail,\n",
        "            title_md,\n",
        "            published_md,\n",
        "            description_md,\n",
        "            category_md,\n",
        "            transcript_accordion,  # show/hide transcript accordion\n",
        "            transcript_md,\n",
        "            comment_count_accordion,  # show/hide comment count stats\n",
        "            comment_count_md\n",
        "        ]\n",
        "    )\n",
        "\n",
        "\n",
        "css = \"\"\"\n",
        ".scroll-text-box {\n",
        "    max-height: 300px;\n",
        "    overflow-y: auto;\n",
        "    white-space: pre-wrap;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "demo.launch(css=css, debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S9ubcANAni8e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NjwWJC2MqNy1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}